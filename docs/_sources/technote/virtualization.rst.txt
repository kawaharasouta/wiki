====================
仮想環境に関する内容
====================

※ここでの話は基本的にKVM/QEMUの話が前提．そうでない場合は注釈をつける．※

ハイパーバイザに関して
======================

そのうちハイパーバイザに関するVMin/exitとかnonrootとかintel-VTとかそこらへんのことを書くけど今は書いてない


準仮想化(paravirtualization)とvirtioに関して
=====================================================

すでに述べた(ことになっている)通り，通常の仮想化の際はハードウェアに対するIOは全てハードウェアのシミュレーションが行われる．
これにより，従来のドライバをそのまま使用することができるため，可用性?を担保できる．このような物を完全仮想化とかいう．
ところでこのハードウェアをエミュレートする方式は，ハードウェア操作のたびにVMexitが発生してしまうため，効率的ではない．
そこで，エミュレーションによる性能低下を図るものが準仮想化(paravirtualization)であり，
その(Linuxでの)フレームワークがvirtioである．

完全仮想化が遅い理由
---------------------

ハードウェア操作はその主たる部分がレジスタ操作である．仮想環境でもそれは同じであり，ハードウェア操作の際には
数回のレジスタ操作が行われる．
また，ハードウェアをエミュレーションするとは，ハードウェアの持つべきレジスタ群を持ちそれらの動作に関する定義を持つことである．
ところで，ハードウェアのエミュレーションはQEMUによって行われる．QEMUはホストOS上の(ユーザ)プロセスである．
ホストOSがハードウェア操作を行う場合，ホストのメモリ空間にアクセスすることになるため，
VMexitしてハイパーバイザにハードウェアレジスタアクセスをエミュレートしてもらう必要があり，
エミュレートが終わるとVMenterされ動作がゲストOSに帰ってくる．
この一連の動作は仮想環境でのみ起こる操作であるため，実ハードウェアとの性能差が出ます．

(以下，一般的なハイパーバイザ環境でのIO性能低下についての引用)

VMX non-root mode・VMX root mode間のモード遷移にかかるコスト
````````````````````````````````````````````````````````````

ハードウェアレジスタアクセス時のVMExitとゲスト再開時のVMEntryでは，
それぞれVMX non-root modeとVMX root modeの間でモード遷移が発生します．
この遷移のコストはCPUの進化に伴い小さくなってきているものの，VMExit・VMEntryにそれぞれ1000サイクルほど消費します．

デバイスエミュレーションの呼び出しにかかるコスト
`````````````````````````````````````````````````

多くの場合，ハイパーバイザのデバイスエミュレータはユーザプロセス上で動作しています．
このため，ハードウェアレジスタアクセスをエミュレートするにはカーネルモードからユーザモードへ遷移し，
エミュレーションを行ってからカーネルモード へ戻ってくる必要があります．

また，ユーザプロセスはプロセススケジューラが適切と判断したタイミングで実行されるため，
VMExit直後にデバイスエミュレータのプロセスが実行される保証はありません．

同様に，ゲスト再開のVMEntryについてもデバイスエミュレーション終了直後に行われる保証はなく，
スケジューリング待ちになる可能性もあります．

また，たいていの完全仮想化デバイスでは一度のI/Oに複数回レジスタアクセスを行う必要があります
(たとえば，あるNICの受信処理では5〜6回のレジスタアクセスが必要になります)．
レジスタアクセスを行うたびに，上述の処理が発生し，大きなコストがかかります．
高速なI/Oが求められるデバイスの場合には，ここが性能上のボトルネックになります．

(以上，ハイパーバイザの作り方～ちゃんと理解する仮想化技術～ 第１１回 virtioによる準仮想化デバイス 
その１「virtioの概要とVirtio PCI」より引用)

virtioの概要
==============

virtio_pciっていうのがなんかPCIデバイスをエミュレートしてる見たいな立ち回りをする感じ．
実際のデータやりとりはvirtio ringっていうメモリ領域でやる．これはshared memory空間にいて，
完全仮想化の場合はデータのやりとり(つまりレジスタ操作)の際に必ず逐一VMExitを発生させる必要があったけど，
共有メモリだからそれがないはず．
virtqueueはどこだ．








vhostとは
==========

この文章はかなりの不確実な成分を含んでいるのであとで必ず書き直す．
virtioと並んでvhostという準仮想ドライバがある．
virtioはバックエンドにQEMUを用いるが，vhostはカーネル空間にバックエンドが存在する．
そのため，コンテキストスイッチが少ない．
とりあえず今の所QEMUを使わないvirtioという風に認識している．
でこれのネットワークインタフェース実装がvhost-net
たぶん，vhostはゲストから見るとvirtioとはなんら違いがないんじゃないか．
ゲストから見たらvirtioって見えてそう．多分virtio_pciが見えてるだろうし．


vhostは、ゲストネットワークトラフィックをカーネル側から直接TUNデバイスに直接渡すことにより、上記のプロセスを加速できます。 このモデルでは、QEMUはvirtqueueの直接制御をカーネルドライバーに渡します。
とかって書いてある資料あったけど，これだとQEMUのパスは一応通ってそうなんだけどどうなのこれ．
QEMUのパス通ってるんだったらコンテキスト一応切り替わるからダメな気がするんだけど．

vhost-user
===========

上のvhostのDPDKアプライアンスのための実装としてvhost-userなるものがあるらしい．
カーネル空間を飛ばすみたいな記述もあったんだけどこれってどっちのカーネル空間の事言ってるんですか

vhost-user server と client
------------------------------

DPDK v16.07でvHostユーザークライアントモードが導入され、DPDKの制限に対処しました。これにより、vHostユーザーバックエンド（DPDKを備えたOVSなどのDPDKベースのアプリケーション）がクラッシュまたは再起動した場合、DPDK vHostユーザーポートを備えたVM バックエンドとの接続を確立し、基本的にネットワークの観点からは役に立たないようにします。 vHostユーザークライアントモードはこの問題を解決します。
とりあえずclient使っとけばいいよみたいな感じ．
要は，DPDKアプライアンスがvhost-userの主体?になる(clientはQEMUらしい)とそれがクラッシュした時に再現が大変になるから
serverは別にいてDPDKアプライアンスはclientとしてそれ(serverはQEMU)を利用するような形態にしたということ．

memo
=========

- virtioのゲスト側の実装はVMM側の実装とは別れている．
  ゲストマシンはvirtioのドライバを持っていてかつVMM側でvirtioのバックエンドドライバが動いている必要がある．
- 昔していた誤解
  virtioは準仮想「ネットワークインタフェース」のことだと思っていたが，そうではなく「準仮想ドライバのフレームワーク」．
  それはネットワークインタフェース実装はvirtio-net
